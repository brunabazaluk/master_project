{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "044e91cd-b500-4877-aacd-6da42c3e1f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # set your cuda device\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "import torch\n",
    "import tla\n",
    "from tla import utils\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LogitsProcessorList\n",
    "\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e40cdcf4-d1ae-46b7-80ff-9b6644025afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bazaluk/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "090028a85fbe4580a54d4e4994f67990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bazaluk/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/tulu-2-7b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"allenai/tulu-2-7b\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "747016f1-5a76-47c5-b6d0-11a1c17ace80",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "dfa = {\n",
    "    \"edges\": [\n",
    "    (0, 2, utils.populate_edge(['X', 'Y'], vocab_size, tokenizer)),\n",
    "    (0, 1, utils.populate_edge(['V'], vocab_size, tokenizer)),\n",
    "    (1, 2, utils.populate_edge(['1', '2', '3', '4', '5'], vocab_size, tokenizer)),\n",
    "    (2, 3, utils.populate_edge(['->'], vocab_size, tokenizer)),\n",
    "    (3, 4, utils.populate_edge(['V'], vocab_size, tokenizer)),\n",
    "    (3, 5, utils.populate_edge(['X','Y'], vocab_size, tokenizer)),\n",
    "    (4, 5, utils.populate_edge(['1', '2', '3', '4', '5'], vocab_size, tokenizer)),\n",
    "    (5, 0, utils.populate_edge([','], vocab_size, tokenizer)),\n",
    "    ],\n",
    "    \"initial_state\": 0,\n",
    "    \"accept_states\": set([5]),\n",
    "    \"current_state\": 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87d1ee79-cc4b-4b5d-ae74-323aafa10cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('/media/data/bazaluk/ctrlg_tulu2/data.csv')\n",
    "data['pred_graphs'] = ''\n",
    "\n",
    "prefix = data['prefix'].iloc[100]\n",
    "d_prompt = data['prompt'].iloc[100]\n",
    "#prefix='output a sequence of \"A\"s: '\n",
    "#d_prompt='output one sequence with the letter \"A\". For example: \"AAAA\"'\n",
    "suffix = '</s>'\n",
    "soft_constraint = '' # use empty string for no soft constraint\n",
    "prompt = f'<|user|>\\n\"{d_prompt}<|endoftext|>\"{soft_constraint}:\\n{prefix}\\n<|assistant|>\\n'\n",
    "\n",
    "\n",
    "prefix_ids = tokenizer.encode(prefix)[1:]\n",
    "suffix_ids = tokenizer.encode(suffix)[1:]\n",
    "prompt_ids = tokenizer.encode(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "daf4d1fd-204b-46e4-9bf3-60fe3b2a4784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'V1 -> X1 , , V3 -> X'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_logits_processor = utils.CustomLogitsProcessor(dfa)\n",
    "\n",
    "min_new_tokens=1\n",
    "max_new_tokens=40\n",
    "eos_token_id = tokenizer.encode('<\\s>')[1]\n",
    "# set the hmm_batch_size & temperature\n",
    "beam_size = 8 # sample 128 sequences\n",
    "temperature = 0.7\n",
    "\n",
    "# generate with sampling, temperature=0.7\n",
    "input_ids = torch.tensor([prompt_ids], device=device)\n",
    "outputs = model.generate(\n",
    "        input_ids=input_ids, do_sample=True,\n",
    "        num_return_sequences=beam_size, \n",
    "        min_new_tokens=min_new_tokens, max_new_tokens=max_new_tokens,\n",
    "        logits_processor=LogitsProcessorList([custom_logits_processor]),\n",
    "        pad_token_id=eos_token_id,\n",
    "        renormalize_logits=True,\n",
    "    )\n",
    "\n",
    "generated_ids = tla.extract_generated_ids(outputs.tolist(), prompt_ids, suffix_ids, eos_token_id)\n",
    "tokenizer.decode(generated_ids[0])\n",
    "#tokenizer.decode(generated_ids[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f5c808b-72d4-4676-bcc9-03bba93e37b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'V1->V3,V1->X,X->Y,V3->Y'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['step1'].iloc[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3b1cd07-5f9d-458e-bca0-631865202d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'edges': [(0, 2, array([False, False, False, ..., False, False, False])),\n",
       "  (0, 1, array([False, False, False, ..., False, False, False])),\n",
       "  (1, 2, array([False, False, False, ..., False, False, False])),\n",
       "  (2, 3, array([False, False, False, ..., False, False, False])),\n",
       "  (3, 4, array([False, False, False, ..., False, False, False])),\n",
       "  (3, 5, array([False, False, False, ..., False, False, False])),\n",
       "  (4, 5, array([False, False, False, ..., False, False, False])),\n",
       "  (5, 0, array([False, False, False, ..., False, False, False]))],\n",
       " 'initial_state': 0,\n",
       " 'accept_states': {5},\n",
       " 'current_state': 5}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2a07b37-b3f3-47fc-80c6-d5b368314247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Extract the causal graph: Identify the causal graph that depicts the relationships in the scenario. Use V1 to represent gender. Use X to represent smoking. Use V3 to represent tar deposit. Use Y to represent lung cancer. The diagram should simply consist of edges denoted in \"var1 -> var2\" format, separated by commas.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['prompt'].iloc[1000]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
