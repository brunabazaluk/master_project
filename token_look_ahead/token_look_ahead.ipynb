{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "044e91cd-b500-4877-aacd-6da42c3e1f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # set your cuda device\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "import torch\n",
    "import tla\n",
    "from tla import utils\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LogitsProcessorList\n",
    "\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e40cdcf4-d1ae-46b7-80ff-9b6644025afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bazaluk/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2946b2d253b41e0a27d0fa5230776a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bazaluk/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/tulu-2-7b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"allenai/tulu-2-7b\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "747016f1-5a76-47c5-b6d0-11a1c17ace80",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "dfa = {\n",
    "    \"edges\": [\n",
    "    (0, 2, utils.populate_edge(['X', 'Y'], vocab_size, tokenizer)),\n",
    "    (0, 1, utils.populate_edge(['V'], vocab_size, tokenizer)),\n",
    "    (1, 2, utils.populate_edge(['1', '2', '3', '4', '5'], vocab_size, tokenizer)),\n",
    "    (2, 3, utils.populate_edge(['->'], vocab_size, tokenizer)),\n",
    "    (3, 4, utils.populate_edge(['V'], vocab_size, tokenizer)),\n",
    "    (3, 5, utils.populate_edge(['X','Y'], vocab_size, tokenizer)),\n",
    "    (4, 5, utils.populate_edge(['1', '2', '3', '4', '5'], vocab_size, tokenizer)),\n",
    "    (5, 0, utils.populate_edge([','], vocab_size, tokenizer)),\n",
    "    ],\n",
    "    \"initial_state\": 0,\n",
    "    \"accept_states\": set([5]),\n",
    "    \"current_state\": 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87d1ee79-cc4b-4b5d-ae74-323aafa10cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.vocab_size\n",
    "import pandas as pd\n",
    "data = pd.read_csv('/media/data/bazaluk/data_pairs_gen.csv')\n",
    "data['pred_graphs'] = ''\n",
    "\n",
    "prefix = data['prefix'].iloc[2000]\n",
    "d_prompt = data['prompt'].iloc[2000]\n",
    "#prefix='output a sequence of \"A\"s: '\n",
    "#d_prompt='output one sequence with the letter \"A\". For example: \"AAAA\"'\n",
    "suffix = '</s>'\n",
    "soft_constraint = '' # use empty string for no soft constraint\n",
    "prompt = f'<|user|>\\n\"{d_prompt}<|endoftext|>\"{soft_constraint}:\\n{prefix}\\n<|assistant|>\\n'\n",
    "\n",
    "\n",
    "prefix_ids = tokenizer.encode(prefix)[1:]\n",
    "suffix_ids = tokenizer.encode(suffix)[1:]\n",
    "prompt_ids = tokenizer.encode(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "daf4d1fd-204b-46e4-9bf3-60fe3b2a4784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X -> V2 , V2 -> Y\n",
      "---\n",
      "X -> V1 , V1 -> Y , V2 -> V1 , V2 -> V2 , V1 -> V2 , Y -> Y\n",
      "---\n",
      "X -> V2\n"
     ]
    }
   ],
   "source": [
    "#custom_logits_processor = utils.CustomLogitsProcessor(dfa)\n",
    "\n",
    "min_new_tokens=1\n",
    "max_new_tokens=40\n",
    "eos_token_id = tokenizer.encode('<\\s>')[1]\n",
    "# set the hmm_batch_size & temperature\n",
    "beam_size = 8 # sample 128 sequences\n",
    "temperature = 0.7\n",
    "prompt_size = len(prompt_ids)\n",
    "\n",
    "custom_logits_processor = utils.ComplexLogitsProcessor(vocab_size, beam_size, prompt_size)\n",
    "\n",
    "# generate with sampling, temperature=0.7\n",
    "input_ids = torch.tensor([prompt_ids], device=device)\n",
    "outputs = model.generate(\n",
    "        input_ids=input_ids, do_sample=True,\n",
    "        num_return_sequences=beam_size, \n",
    "        min_new_tokens=min_new_tokens, max_new_tokens=max_new_tokens,\n",
    "        logits_processor=LogitsProcessorList([custom_logits_processor]),\n",
    "        pad_token_id=eos_token_id,\n",
    "        renormalize_logits=True,\n",
    "    )\n",
    "\n",
    "generated_ids = tla.extract_generated_ids(outputs.tolist(), prompt_ids, suffix_ids, eos_token_id)\n",
    "print(tokenizer.decode(generated_ids[0]))\n",
    "print('---')\n",
    "print(tokenizer.decode(generated_ids[1]))\n",
    "print('---')\n",
    "print(tokenizer.decode(generated_ids[2]))\n",
    "#print(tokenizer.decode(generated_ids[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f5c808b-72d4-4676-bcc9-03bba93e37b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'V1->X,V1->Y,X->Y'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['step1'].iloc[2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3b1cd07-5f9d-458e-bca0-631865202d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prompt_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c2a07b37-b3f3-47fc-80c6-d5b368314247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Extract the causal graph: Identify the causal graph that depicts the relationships in the scenario. Use V1 to represent gender. Use X to represent smoking. Use V3 to represent tar deposit. Use Y to represent lung cancer. The diagram should simply consist of edges denoted in \"var1 -> var2\" format, separated by commas.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['prompt'].iloc[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "78e27800-87fd-419a-b25a-c76a6fb033a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/media/data/bazaluk/data_pairs_gen.csv')\n",
    "data['pred_graphs'] = ''\n",
    "\n",
    "df=pd.DataFrame(columns=['prompt','prefix','step1','pred_graphs'])\n",
    "df.to_csv('/media/data/bazaluk/token_look_ahead/data.csv')\n",
    "\n",
    "for i in range(10):\n",
    "    prefix = data['prefix'].iloc[i]\n",
    "    d_prompt = data['prompt'].iloc[i]\n",
    "    #prefix='output a sequence of \"A\"s: '\n",
    "    #d_prompt='output one sequence with the letter \"A\". For example: \"AAAA\"'\n",
    "    suffix = '</s>'\n",
    "    soft_constraint = '' # use empty string for no soft constraint\n",
    "    prompt = f'<|user|>\\n\"{d_prompt}<|endoftext|>\"{soft_constraint}:\\n{prefix}\\n<|assistant|>\\n'\n",
    "\n",
    "    prefix_ids = tokenizer.encode(prefix)[1:]\n",
    "    suffix_ids = tokenizer.encode(suffix)[1:]\n",
    "    prompt_ids = tokenizer.encode(prompt)\n",
    "\n",
    "    custom_logits_processor = utils.ComplexLogitsProcessor(vocab_size)\n",
    "\n",
    "    min_new_tokens=1\n",
    "    max_new_tokens=40\n",
    "    eos_token_id = tokenizer.encode('<\\s>')[1]\n",
    "    # set the hmm_batch_size & temperature\n",
    "    beam_size = 8 # sample 128 sequences\n",
    "    temperature = 0.7\n",
    "\n",
    "    # generate with sampling, temperature=0.7\n",
    "    input_ids = torch.tensor([prompt_ids], device=device)\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids, do_sample=True,\n",
    "        num_return_sequences=beam_size, \n",
    "        min_new_tokens=min_new_tokens, max_new_tokens=max_new_tokens,\n",
    "        logits_processor=LogitsProcessorList([custom_logits_processor]),\n",
    "        pad_token_id=eos_token_id,\n",
    "        renormalize_logits=True,\n",
    "    )\n",
    "\n",
    "    generated_ids = tla.extract_generated_ids(outputs.tolist(), prompt_ids, suffix_ids, eos_token_id)\n",
    "\n",
    "    data.at[i,'pred_graphs'] = tokenizer.decode(generated_ids[0])\n",
    "    data[['Unnamed: 0','prompt','prefix','step1','pred_graphs']][i:i+1].to_csv('/media/data/bazaluk/token_look_ahead/data.csv', mode='a', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bbf30d22-f3d3-413b-aea3-bf2af5a94db4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>prompt</th>\n",
       "      <th>prefix</th>\n",
       "      <th>step1</th>\n",
       "      <th>pred_graphs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>For patients who are young and pay a low hospi...</td>\n",
       "      <td>Identify the causal graph that depicts the re...</td>\n",
       "      <td>V1-&gt;X,V1-&gt;Y,X-&gt;Y</td>\n",
       "      <td>V1 -&gt; X ,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The overall probability of citrus intake is 2%...</td>\n",
       "      <td>Identify the causal graph that depicts the re...</td>\n",
       "      <td>X-&gt;V2,V2-&gt;Y</td>\n",
       "      <td>X -&gt; V2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>We know that liking spicy food and the chain-s...</td>\n",
       "      <td>Identify the causal graph that depicts the re...</td>\n",
       "      <td>X-&gt;Y,V2-&gt;Y</td>\n",
       "      <td>V2 -&gt; X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>For nonsmokers, the probability of freckles is...</td>\n",
       "      <td>Identify the causal graph that depicts the re...</td>\n",
       "      <td>X-&gt;V2,X-&gt;Y,V2-&gt;Y</td>\n",
       "      <td>X -&gt; V2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>For individuals who are not male and applicant...</td>\n",
       "      <td>Identify the causal graph that depicts the re...</td>\n",
       "      <td>X-&gt;V3,V2-&gt;V3,X-&gt;Y,V2-&gt;Y,V3-&gt;Y</td>\n",
       "      <td>V2 -&gt; X1 ,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>For farms with reduced crop yield per acre, th...</td>\n",
       "      <td>Identify the causal graph that depicts the re...</td>\n",
       "      <td>V1-&gt;X,V2-&gt;X,V1-&gt;Y,X-&gt;Y</td>\n",
       "      <td>X1 -&gt; V2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>We know that CEO's decision to fire the employ...</td>\n",
       "      <td>Identify the causal graph that depicts the re...</td>\n",
       "      <td>V1-&gt;V3,V1-&gt;X,X-&gt;Y,V3-&gt;Y</td>\n",
       "      <td>V1 -&gt; V3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>For unvaccinated individuals, the probability ...</td>\n",
       "      <td>Identify the causal graph that depicts the re...</td>\n",
       "      <td>X-&gt;V3,X-&gt;V2,V2-&gt;Y,V3-&gt;Y</td>\n",
       "      <td>V -&gt; V3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>We know that solar eclipse and heavy traffic c...</td>\n",
       "      <td>Identify the causal graph that depicts the re...</td>\n",
       "      <td>X-&gt;Y,V2-&gt;Y</td>\n",
       "      <td>V2 -&gt; Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>The overall probability of smoking is 81%. For...</td>\n",
       "      <td>Identify the causal graph that depicts the re...</td>\n",
       "      <td>X-&gt;V2,V2-&gt;Y</td>\n",
       "      <td>X -&gt; V1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                             prompt  \\\n",
       "0           0  For patients who are young and pay a low hospi...   \n",
       "1           1  The overall probability of citrus intake is 2%...   \n",
       "2           2  We know that liking spicy food and the chain-s...   \n",
       "3           3  For nonsmokers, the probability of freckles is...   \n",
       "4           4  For individuals who are not male and applicant...   \n",
       "5           5  For farms with reduced crop yield per acre, th...   \n",
       "6           6  We know that CEO's decision to fire the employ...   \n",
       "7           7  For unvaccinated individuals, the probability ...   \n",
       "8           8  We know that solar eclipse and heavy traffic c...   \n",
       "9           9  The overall probability of smoking is 81%. For...   \n",
       "\n",
       "                                              prefix  \\\n",
       "0   Identify the causal graph that depicts the re...   \n",
       "1   Identify the causal graph that depicts the re...   \n",
       "2   Identify the causal graph that depicts the re...   \n",
       "3   Identify the causal graph that depicts the re...   \n",
       "4   Identify the causal graph that depicts the re...   \n",
       "5   Identify the causal graph that depicts the re...   \n",
       "6   Identify the causal graph that depicts the re...   \n",
       "7   Identify the causal graph that depicts the re...   \n",
       "8   Identify the causal graph that depicts the re...   \n",
       "9   Identify the causal graph that depicts the re...   \n",
       "\n",
       "                           step1 pred_graphs  \n",
       "0               V1->X,V1->Y,X->Y   V1 -> X ,  \n",
       "1                    X->V2,V2->Y     X -> V2  \n",
       "2                     X->Y,V2->Y     V2 -> X  \n",
       "3               X->V2,X->Y,V2->Y     X -> V2  \n",
       "4  X->V3,V2->V3,X->Y,V2->Y,V3->Y  V2 -> X1 ,  \n",
       "5         V1->X,V2->X,V1->Y,X->Y    X1 -> V2  \n",
       "6        V1->V3,V1->X,X->Y,V3->Y    V1 -> V3  \n",
       "7        X->V3,X->V2,V2->Y,V3->Y     V -> V3  \n",
       "8                     X->Y,V2->Y     V2 -> Y  \n",
       "9                    X->V2,V2->Y     X -> V1  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('/media/data/bazaluk/token_look_ahead/data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
